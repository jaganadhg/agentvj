{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Optional, Union, Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DummyLLM:\n",
    "    \"\"\"\n",
    "    A dummy LLM implementation that returns random responses\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, temperature: float = 0.7):\n",
    "        self.temperature = temperature\n",
    "        self._responses = [\n",
    "            \"I think we should use the calculator tool for this task.\",\n",
    "            \"Let me search through the available tools.\",\n",
    "            \"Based on my analysis, we should proceed step by step.\",\n",
    "            \"I recommend using the following approach...\",\n",
    "            \"The solution requires mathematical computation.\",\n",
    "            \"Let's break this problem down into smaller parts.\",\n",
    "        ]\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        system_message: Optional[str] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ) -> Dict[str, Union[str, float]]:\n",
    "        \"\"\"\n",
    "        Generate a random response with metadata\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt\n",
    "            system_message (Optional[str]): System message to guide generation\n",
    "            temperature (Optional[float]): Override default temperature\n",
    "\n",
    "        Returns:\n",
    "            Dict containing response text and metadata\n",
    "        \"\"\"\n",
    "        # Use random confidence score between 0.3 and 0.9\n",
    "        confidence = random.uniform(0.3, 0.9)\n",
    "\n",
    "        return {\n",
    "            \"text\": random.choice(self._responses),\n",
    "            \"confidence\": confidence,\n",
    "            \"tokens_used\": random.randint(10, 50),\n",
    "            \"finish_reason\": \"stop\",\n",
    "        }\n",
    "\n",
    "    def batch_generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        system_message: Optional[str] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "    ) -> List[Dict[str, Union[str, float]]]:\n",
    "        \"\"\"\n",
    "        Generate multiple random responses\n",
    "\n",
    "        Args:\n",
    "            prompts (List[str]): List of input prompts\n",
    "            system_message (Optional[str]): System message to guide generation\n",
    "            temperature (Optional[float]): Override default temperature\n",
    "\n",
    "        Returns:\n",
    "            List of response dictionaries\n",
    "        \"\"\"\n",
    "        return [\n",
    "            self.generate(prompt, system_message, temperature) for prompt in prompts\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for DummyLLM class\n",
    "def test_dummy_llm_generate():\n",
    "    llm = DummyLLM()\n",
    "    response = llm.generate(\"Test prompt\")\n",
    "    assert \"text\" in response\n",
    "    assert \"confidence\" in response\n",
    "    assert \"tokens_used\" in response\n",
    "    assert \"finish_reason\" in response\n",
    "    assert 0.3 <= response[\"confidence\"] <= 0.9\n",
    "    assert 10 <= response[\"tokens_used\"] <= 50\n",
    "\n",
    "def test_dummy_llm_batch_generate():\n",
    "    llm = DummyLLM()\n",
    "    prompts = [\"Prompt 1\", \"Prompt 2\"]\n",
    "    responses = llm.batch_generate(prompts)\n",
    "    assert len(responses) == len(prompts)\n",
    "    for response in responses:\n",
    "        assert \"text\" in response\n",
    "        assert \"confidence\" in response\n",
    "        assert \"tokens_used\" in response\n",
    "        assert \"finish_reason\" in response\n",
    "        assert 0.3 <= response[\"confidence\"] <= 0.9\n",
    "        assert 10 <= response[\"tokens_used\"] <= 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for AgentMemory class\n",
    "def test_agent_memory_add_step():\n",
    "    memory = AgentMemory()\n",
    "    step = {\"action\": \"test_action\", \"result\": \"test_result\"}\n",
    "    memory.add_step(step)\n",
    "    assert len(memory.steps) == 1\n",
    "    assert memory.steps[0] == step\n",
    "\n",
    "def test_agent_memory_reset():\n",
    "    memory = AgentMemory()\n",
    "    step = {\"action\": \"test_action\", \"result\": \"test_result\"}\n",
    "    memory.add_step(step)\n",
    "    memory.reset()\n",
    "    assert len(memory.steps) == 0\n",
    "    assert len(memory.conversation) == 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test cases for BaseAgent class\n",
    "class TestTool:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "\n",
    "    def execute(self, input):\n",
    "        return f\"Executed {input}\"\n",
    "\n",
    "class TestAgent(BaseAgent):\n",
    "    def generate_action(self, task: str) -> dict:\n",
    "        return {\"tool\": \"test_tool\", \"input\": task}\n",
    "\n",
    "    def execute_action(self, action: dict) -> Any:\n",
    "        tool = self.tools[action[\"tool\"]]\n",
    "        return tool.execute(action[\"input\"])\n",
    "\n",
    "def test_base_agent_run():\n",
    "    tools = [TestTool(\"test_tool\")]\n",
    "    model = lambda x: x  # Dummy model for demonstration\n",
    "    agent = TestAgent(tools=tools, model=model)\n",
    "    task = \"Test task\"\n",
    "    result = agent.run(task)\n",
    "    assert result == \"Executed Test task\"\n",
    "\n",
    "def test_base_agent_update_memory():\n",
    "    tools = [TestTool(\"test_tool\")]\n",
    "    model = lambda x: x  # Dummy model for demonstration\n",
    "    agent = TestAgent(tools=tools, model=model)\n",
    "    action = {\"tool\": \"test_tool\", \"input\": \"Test input\"}\n",
    "    result = \"Test result\"\n",
    "    agent.update_memory(action, result)\n",
    "    assert len(agent.memory.steps) == 1\n",
    "    assert agent.memory.steps[0][\"action\"] == action\n",
    "    assert agent.memory.steps[0][\"result\"] == result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
